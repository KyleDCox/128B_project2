 \documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\small\bfseries #1}\hskip \labelsep {\small\bfseries #2}]}{\end{trivlist}}
\newenvironment{epart}[2][Part]{\begin{trivlist}
\item[\hskip \labelsep {\footnotesize\bfseries #1}\hskip \labelsep {\footnotesize\bfseries #2}]}{\end{trivlist}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
 
\begin{document}
 
\title{MAT 128B Project 2}
\author{Steven An, Kyle Cox}
\date{March 14, 2018}
\maketitle

\subsection*{Distribution of work}


\subsection*{Part (i)}
Followed instructions.

\subsection*{Part (ii)}
Done as requested.  
See attached.

\subsection*{Part (iii)}
Let $x$ be $NET$.
We have 
\[OUT = F(x) = \frac{1}{1+e^{-x}}\]
and
\[F^\prime(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{1-1+e^{-x}}{(1+e^{-x})^2} = \frac{1+e^{-x}}{(1+e^{-x})^2} -\frac{1}{(1+e^{-x})^2} = F(x)(1-F(x))\]
which was to be shown.
When $x$ is large and positive, then $F(x)$ will be close to 1 because as $x\rightarrow\infty$, $e^{-x}\rightarrow 0$.
On the other hand, when $x$ is large and negative, $F(x)$ will tend to zero.
This is because $e^{-x}$ is very large when $x\ll 0$.
Therefore, as the numerator is constant, $F(x)$ will tend to zero.
Another example of a function that can be used is $-\arctan\left(x\right) + \pi/2$.
The function is both differentiable and uniformly bounded.
However, here, when $x$ is negative, the function approaches its upper bound and \textit{vice versa}.
The relation between the input and output need not be standard.
One may consider sinusoidal functions like $\sin(x)+1 \text{ and }\cos(x)+1$.
These are bounded, differentiable, and always positive, but are oscillatory.
For strictly monotonic functions, either $F$ approaches its lower or upper bound when $x$ is large and positive and \textit{vice versa}.
\subsection*{Part (iv)}
This function returns a matrix that contains the value of the neurons at each neural network for each layer except for the input layer.
This is manually done for the first hidden layer because the first set of weights is not in the three dimensional array.
The appropriate matrices and neuron values are chosen, put as arguments into the function from part (iii) and the results recorded.

\subsection*{Part (v)}
This function creates the weights of the connections between layers.
On the advice of a friend, values of the weights were chosen to be distributed on the interval [-0.01, 0.01].
To save memory, the matrix of weights going from the input layer to the first hidden layer has its own matrix.
For all other layers, there is a three dimension matrix where each layer stores the weights between each layer.
For our weight matrix $W\in\R^{m\times n\times k}$, $w_{ijk}$ is the weight from the $j^{th}$ node of the $k^{th}$ hidden layer to the $i^{th}$ node of the $(k+1)^{th}$ hidden layer. We consider our output layer as the last hidden layer in reference to this matrix.
This allows for less confusing notation in part (iv).
$NumHidden$ is the number of hidden layers.
$NumNeurons$ is a vector containing the number of neurons in each respective hidden layer (not including our output layer).
The length of $NumNeurons$ is $NumHidden+1$ because the number of neurons in the output layer is 10 and there must be weights going to the output layer.
The range of weights is the same as before.
\subsection*{Part (vi)}
The forward propagation is done with a call to the function in part (iv).
It should be noted that the error computation with the update of 
weights as described in the prompt is incorrect.
This was verified by the fact that every neural network we generated from those instructions failing to confirm a single digit when only trained for that digit.
Therefore, we chose to compute $ERROR=TARGET-OUTPUT$ where \emph{TARGET}, for the $(i-1)^\text{th}$, is the standard basis vector, $e_i$.
We keep the original weight update equation, as now there are signs.
Before, if there was an error, and the output of the neural network was too high, all that would happen is that the weights would increase.
The $\delta$ vector is computed all at once.
here, $OUT_k$ is the output layer.
This is done by \[\delta = OUT_k \circ (1-OUT_k) \circ ERROR\]
where $\circ$ is the Hadamard product.
Then, the matrix \[\Delta w_k = \eta \cdot \delta \cdot OUT_{k-1}^T.\]
NOTE: The weight change matrix is the \textbf{transpose} of what is described in the prompt.  
This is because we chose to implement the transpose of the weight matrix.

We first note that the index of summation for the $\delta$ computation in the hidden layer weight adjustment is $q$ and not $k$ as typed.
Then, it is clear that the first part of the multiplication is $O_j\circ (I-O_j)$.
It shall now be shown that the expression provided is incorrect.
The sum we'd like to convert to matrix form for the $k^{th}$ layer is 
\[\sum_{q}\delta_q w_{pq}.\]
Let $D_k$ be all $\delta$'s for layer k.
That is, for row $p$ of matrix $W$, multiply the $q^{th}$ element of $\delta$ to the element in the $q^{th}$ column (and $p^{th}$ row of $W$.
So the corresponding expression is $(W\delta)_p$, the $p^{th}$ element of that vector.
Then, for the general case, we have \[D_j = WD_k \circ O_j\circ (I-O_j)\]
where $I$ is the column vector of ones (as used in the prompt).
Since the weight matrix for our program is $\hat W =W^T$, the implementation found is actually 
\[D_j = (D_k^T\hat W)^T\circ O_j\circ (I-O_j).\]
Then, the same equation as above is used to compute \(\Delta w\).

\subsection*{Part (vii)}
To train our network, we run ever image through the network, a different digit every iteration, and perform the reverse pass.
In order to eliminate bias, the same amount of images for each digit was used.

To check if the neural net guesses the correct digit, we check to see if the position of the max element of the output layer is equal to $1+\text{the correct digit}$ (because positioning in matlab begins at 1, but our digits begin at 0). 
If this is true, then we add one to our count of correct guesses, then divide the total number of correct guesses by the total number of test inputs.

After training the system we first tested a regime of two hidden layer with 100 and 50 neurons respectively. 
We denote this as `100 - 50, 5421x10' where the second term is the number of images from each digit that was used for training.
This network structure was tested by varying the number of training iterations (one iteration is training through the entire training set once). 
We tested this 20 times for 10 iterations and 15 iterations each, and once for 20 iterations (as the latter took too much time).

It should be said that in general, the higher the iteration count, the smaller the error.
However, if the neural network becomes overfitted to the training data, the error percentage rises.
It is surmised that with 20 iterations, the neural network becomes overfitted to the training data and thus has a lower success rate with the test data. 

The results are provided in the tables below. 
The Training Error \% denotes the percent of the time the neural net guesses the incorrect digit when the training set is used as an input. 
Similarly, Test Error \% represents the same for when the test set is used as an input.

\begin{table}[h]
	\begin{center}
		\caption{Error Stats for 100 - 50, 5421x10}
		\label{tab:table1}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}\textbf{Iterations} & \multicolumn{4}{c|}{\textbf{Training Error \%}} &  \multicolumn{4}{c|}{\textbf{Test Error\%}} \\
			\cline{2-9}
			&Min & Mean & Max & Variance & Min & Mean & Max& Variance\\
			\hline
			10 & 3.99 & 5.67 & 7.35 & 0.818 & 4.7 & 6.41 & 8.53 & 0.947\\
			\hline
			15 & 2.94 & 3.95 & 6.51 & 0.641 & 4.22 & 5.13 & 6.61 & 0.314 \\
			\hline
			20 & N/a & 2.7 & N/a & N/a & N/a & 6.08 & N/a & N/a\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

We see that from our results of training the neural network with 10 iterations 20 times that indeed, all aspects of error decreased.
For this network structure, the value of $\eta$ did not really matter because multiple values were tested before these 41 tests were run and the error rate were not very different from the ones seen here.

\begin{table}[h]
	\begin{center}
		\caption{Error Stats for 50 - 25, 5421x10}
		\label{tab:table1}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}\textbf{Iterations} & \multicolumn{4}{c|}{\textbf{Training Error \%}} &  \multicolumn{4}{c|}{\textbf{Test Error\%}} \\
			\cline{2-9}
			&Min & Mean & Max & Variance & Min & Mean & Max& Variance\\
			\hline
			10 & 4.89 & 22.43 & 69.54 & 488.41 & 5.82 & 22.97 & 69.32 & 479.61\\
			\hline
			15 & 3.71 & 6.89 & 20.41 & 19.9 & 4.61 &7.77 & 20.23 & 18.38\\
			\hline
			20 & N/a & 3.41 & N/a & N/a & N/a & 5.0 & N/a & N/a\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

For this structure, we see that the decrease in neurons negatively affected the success rate.
Even though the best result was close to that of the 100 - 50 structure, the variance was orders higher for 10 and 15 iterations.
Perhaps there is a balance between training the network and not over-training it.
The boundary between the two, would, in our speculations, be unknowable because we do not fully understand how the sigmoid function and weights represent the network's cognition, if it has any.
Regardless, our original claim of error decreasing as training iterations increase is further corroborated by this smaller neural network.

Another hypothesis is that because the neuron count in the first layer is so low, it cannot faithfully represent the `essence' of the digits.
That is, what the neural network represents is too convoluted and  certain properties of the digit cannot be easily assigned to the representation given by the network.
This `bad fitting' of the digits is what contributes to the low accuracy (as compared to 100 50).

The value of $\eta$ did matter for some sets of initial weights when training this network.
For certain weights, if $\eta = 0.01$, then it would only train for the last digit, 9.
This was further supported by testing how the network performed when it was only trained and tested on two digits.
Even for two digits, the network would only be trained for the second one.
Choosing $\eta = 0.025$ fixed the problem and generated the results seen.

\begin{table}[h]
	\begin{center}
		\caption{Error Stats for 100 - 50, 2710x10}
		\label{tab:table1}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}\textbf{Iterations} & \multicolumn{4}{c|}{\textbf{Training Error \%}} &  \multicolumn{4}{c|}{\textbf{Test Error\%}} \\
			\cline{2-9}
			&Min & Mean & Max & Variance & Min & Mean & Max& Variance\\
			\hline
			10 & . & . & . & . & . & . & . & .\\
			\hline
			15 & . & . & . & . & . &. & . & . \\
			\hline
			20 & N/a & . & N/a & N/a & N/a & . & N/a & N/a\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\caption{Error Stats for 50 - 25, 2710x10}
		\label{tab:table1}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}\textbf{Iterations} & \multicolumn{4}{c|}{\textbf{Training Error \%}} &  \multicolumn{4}{c|}{\textbf{Test Error\%}} \\
			\cline{2-9}
			&Min & Mean & Max & Variance & Min & Mean & Max& Variance\\
			\hline
			10 & 40.75 & 62.66 & 90.09 & 212.86 & 43.85 & 63.93 & 89.91 & 188.89\\
			\hline
			15 & . & . & . & . & . &. & . & . \\
			\hline
			20 & N/a & . & N/a & N/a & N/a & . & N/a & N/a\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\caption{Error for 100 - 50 - 25, 5421x10}
		\label{tab:table2}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Iterations} & \textbf{Training Error \%} & \textbf{Test Error \%} \\
			\hline
			10 & 23.97 & 18.30\\
			& 25.99 & 22.88\\
			\hline
			20 & 20.14 & 16.52 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

The neural network was trained with ten iterations, giving the result in the first row.
Then, the same set of weights was trained again to get the result in the third row.
It might be claimed that the more neurons the network has, the better it will be performed.
These results do no give credence to either view for one can claim that if the network were further trained, the accuracy would increase to the mid nineties, but skeptics may say that the network structure inherently guarantees that the accuracy will not be high.
Since we do not know the rate of convergence to the truth for the network, not much can be drawn from the small increase in accuracy when the training set was run through ten more times.

\subsection*{Concluding Remarks}

\subsection*{Raw Data (Error Perc.)}
\begin{table}[h]
	\begin{center}
		\caption{Error for 100 - 50, 5421x10}
		\label{tab:table2}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Iterations} & \textbf{Training Error \%} & \textbf{Test Error \%} \\
			\hline
			10 & 7.35 & 8.53\\
			& 5.81 & 6.9\\
			& 6.17 & 7.2\\
			& 6.11 & 7.43\\
			& 5.53 & 6.47 \\
			& 7.1 & 7.9 \\
			& 4.7 & 5.81 \\
			& 4.45 & 5.03 \\
			& 6.05 & 6.79 \\
			& 6.06 & 6.73 \\
			& 4.67 & 5.45 \\
			& 5.84 & 6.15 \\
			& 5.4 & 6.12 \\
			& 3.99 & 4.7 \\
			& 4.3 & 5.01 \\
			& 5.16 & 5.73 \\
			& 5.96 & 6.32 \\
			& 6.52 & 6.9 \\
			& 5.86 & 6.25 \\
			& 6.42 & 6.81 \\
			\hline
			15 & 3.25 & 5.51\\
			& 3.18 & 5.05\\
			& 3.51 & 5.76\\
			& 5.01 & 5.66\\
			& 3.76 & 4.96\\
			& 4.16 & 5.44\\
			& 3.73 & 4.74\\
			& 3.47 & 4.80\\
			& 2.94 & 4.22\\
			& 3.89 & 5.10\\
			& 4.71 & 5.70\\
			& 4.06 & 5.19\\
			& 3.70 & 4.62\\
			& 3.15 & 4.55\\
			& 3.99 & 5.30\\
			& 4.32 & 5.24\\
			& 3.62 & 4.59\\
			& 4.36 & 5.20\\
			& 6.51 & 6.61\\
			& 3.59 & 4.44\\
			\hline
			20 & 2.7 & 6.08\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\caption{Error for 50 - 25, 5421x10}
		\label{tab:table2}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Iterations} & \textbf{Training Error \%} & \textbf{Test Error \%} \\
			\hline
			10 & 12.04 & 11.89\\
			& 4.89 & 5.82\\
			& 6.51 & 6.57\\
			& 5.28 & 5.94\\
			& 52.17 & 52.13\\
			& 30.26 & 30.65\\
			& 47.76 & 47.92\\
			& 6.86 & 7.16\\
			& 7.03 & 7.57\\
			& 7.94 & 8.67\\
			& 8.45 & 8.82\\
			& 69.54 & 69.22\\
			& 6.04 & 6.59\\
			& 8.34 & 8.73\\
			& 69.52 & 69.32\\
			& 7.72 & 7.85\\
			& 6.64 & 6.87\\
			& 33.69 & 34.60\\
			& 13.01 & 13.20\\
			& 47.87 & 47.95\\
			\hline
			15 & 4.21 & 4.96\\
			& 18.12 & 18.30\\
			& 5.84 & 6.72\\
			& 5.95 & 6.93\\
			& 10.29 & 11.15\\
			& 4.92 & 5.76\\
			& 4.02 & 5.35\\
			& 3.72 & 4.61\\
			& 4.48 & 5.49\\
			& 3.71 & 5.04\\
			& 5.34 & 6.64\\
			& 5.99 & 6.93\\
			& 5.25 & 6.37\\
			& 4.84 & 5.86\\
			& 4.13 & 5.31\\
			& 7.99 & 9.10\\
			& 20.41 & 20.23\\
			& 9.44 & 9.69\\
			& 4.16 & 5.39\\
			& 4.89 & 5.62\\
			\hline
			20 & 3.41 & 5.0 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\begin{center}
		\caption{Error for 50 - 25, 2710x10}
		\label{tab:table2}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Iterations} & \textbf{Training Error \%} & \textbf{Test Error \%} \\
			\hline
			10 & 64.38 & 68.97\\
			& 77.73 & 74.77\\
			& 60.13 & 61.13\\
			& 58.76 & 63.92\\
			& 40.75 & 43.85\\
			& 59.92 & 59.63\\
			& 90.09 & 89.91\\
			& 53.49 & 50.44\\
			& 47.35 & 47.50\\
			& 43.24 & 48.90\\
			& 75.95 & 80.32\\
			& 60.33 & 63.73\\
			& 60.25 & 59.96\\
			& 80.32 & 81.06\\
			& 51.85 & 59.75\\
			& 80.19 & 81.20\\
			& 79.11 & 79.08\\
			& 43.66 & 44.52\\
			& 75.11 & 79.15\\
			& 50.55 & 50.71\\
			\hline
			15 & . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			& . & .\\
			\hline
			20 & 21.78 & 20.03 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\end{document}
