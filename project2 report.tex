\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\small\bfseries #1}\hskip \labelsep {\small\bfseries #2}]}{\end{trivlist}}
\newenvironment{epart}[2][Part]{\begin{trivlist}
\item[\hskip \labelsep {\footnotesize\bfseries #1}\hskip \labelsep {\footnotesize\bfseries #2}]}{\end{trivlist}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
 
\begin{document}
 
\title{MAT 128B Project 2}
\author{Steven An, Kyle Cox}
\date{March 14, 2018}
\maketitle

\subsection*{Distribution of work}


\subsection*{Part (i)}
Followed instructions.

\subsection*{Part (ii)}
Done as requested.  
See attached.

\subsection*{Part (iii)}
Let $x$ be $NET$.
We have 
\[OUT = F(x) = \frac{1}{1+e^{-x}}\]
and
\[F^\prime(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{1-1+e^{-x}}{(1+e^{-x})^2} = \frac{1+e^{-x}}{(1+e^{-x})^2} -\frac{1}{(1+e^{-x})^2} = F(x)(1-F(x))\]
which was to be shown.
When $x$ is large and positive, then $F(x)$ will be close to 1 because as $x\rightarrow\infty$, $e^{-x}\rightarrow 0$.
On the other hand, when $x$ is large and negative, $F(x)$ will tend to zero.
This is because $e^{-x}$ is very large when $x\ll 0$.
Therefore, as the numerator is constant, $F(x)$ will tend to zero.
Another example of a function that can be used is $-\arctan\left(x\right) + \pi/2$.
The function is both differentiable and uniformly bounded.
However, here, when $x$ is negative, the function approaches its upper bound and \textit{vice versa}.
The relation between the input and output need not be standard.
One may consider sinusoidal functions like $\sin(x)+1 \text{ and }\cos(x)+1$.
These are bounded, differentiable, and always positive, but are oscillatory.
For strictly monotonic functions, either $F$ approaches its lower or upper bound when $x$ is large and positive and \textit{vice versa}.
\subsection*{Part (iv)}
This function returns a matrix that contains the value of the neurons at each neural network for each layer except for the input layer.
This is manually done for the first hidden layer because the first set of weights is not in the three dimensional array.
The appropriate matrices and neuron values are chosen, put as arguments into the function from part (iii) and the results recorded.

\subsection*{Part (v)}
This function creates the weights of the connections between layers.
On the advice of a friend, values of the weights were chosen to be distributed on the interval [-0.01, 0.01].
To save memory, the matrix of weights going from the input layer to the first hidden layer has its own matrix.
For all other layers, there is a three dimension matrix where each layer stores the weights between each layer.
For our weight matrix $W\in\R^{m\times n\times k}$, $w_{ijk}$ is the weight from the $j^{th}$ node of the $k^{th}$ hidden layer to the $i^{th}$ node of the $(k+1)^{th}$ hidden layer. We consider our output layer as the last hidden layer in reference to this matrix.
This allows for less confusing notation in part (iv).
$NumHidden$ is the number of hidden layers.
$NumNeurons$ is a vector containing the number of neurons in each respective hidden layer (not including our output layer).
The length of $NumNeurons$ is $NumHidden+1$ because the number of neurons in the output layer is 10 and there must be weights going to the output layer.
The range of weights is the same as before.
\subsection*{Part (vi)}
The forward propagation is done with a call to the function in part (iv).
It should be noted that the error computation with the update of 
weights as described in the prompt is incorrect.
This was verified by the fact that every neural network we generated from those instructions failing to confirm a single digit when only trained for that digit.
Therefore, we chose to compute $ERROR=TARGET-OUTPUT$ where \emph{TARGET}, for the $(i-1)^\text{th}$, is the standard basis vector, $e_i$.
We keep the original weight update equation, as now there are signs.
Before, if there was an error, and the output of the neural network was too high, all that would happen is that the weights would increase.
The $\delta$ vector is computed all at once.
here, $OUT_k$ is the output layer.
This is done by \[\delta = OUT_k \circ (1-OUT_k) \circ ERROR\]
where $\circ$ is the Hadamard product.
Then, the matrix \[\Delta w_k = \eta \cdot \delta \cdot OUT_{k-1}^T.\]
NOTE: The weight change matrix is the \textbf{transpose} of what is described in the prompt.  
This is because we chose to implement the transpose of the weight matrix.

We first note that the index of summation for the $\delta$ computation in the hidden layer weight adjustment is $q$ and not $k$ as typed.
Then, it is clear that the first part of the multiplication is $O_j\circ (I-O_j)$.
It shall now be shown that the expression provided is incorrect.
The sum we'd like to convert to matrix form for the $k^{th}$ layer is 
\[\sum_{q}\delta_q w_{pq}.\]
Let $D_k$ be all $delta$'s for layer k.
That is, for row $p$ of matrix $W$, multiply the $q^{th}$ element of $\delta$ to the element in the $q^{th}$ column (and $p^{th}$ row of $W$.
So the corresponding expression is $(W\delta)_p$, the $p^{th}$ element of that vector.
Then, for the general case, we have \[D_j = WD_k \circ O_j\circ (I-O_j).\]
Since the weight matrix for our program is $\hat W =W^T$, the implementation found is actually 
\[D_j = (D_k^T\hat W)^T\circ O_j\circ (I-O_j).\]
Then, the same equation as above is used to compute \(\Delta w\).
\subsection*{Part (vii)}
To train our network, we run ever image through the network, a different digit every iteration, and perform the reverse pass.
In order to eliminate bias, the same amount of images for each digit was used.\\
To check if the neural net guesses the correct digit, we check to see if the position of the max element of the output layer is equal to $1+\text{the correct digit}$ (because positioning in matlab begins at 1, but out digits begin at 0). If this is true, then we add one to our count of correct guesses, then divide the total number of correct guesses by the total number of test inputs.\\
After training the system we first tested a regime of two hidden layer with 100 and 50 neurons respectively. We denote this regime: 100 - 50. We tested this regime on a varying the number of training iterations (one iteration is training through the entire training set once). We tested this 20 times for 10 iterations and 15 iterations each, and once for 20 iterations. The results are provided in Table 1, below. The Training Error \% denotes the percent of the time the neural net guesses the incorrect digit when the training set is used as an input. Respectively, Test Error \% represents the same for when the test set is used as an input.

\begin{table}[h]
	\begin{center}
		\caption{Error Stats for 100 - 50}
		\label{tab:table1}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multirow{2}{*}\textbf{Iterations} & \multicolumn{2}{c|}{\textbf{Training Error \%}} & \multicolumn{2}{c|}{\textbf{Test Error\%}} \\
			\cline{2-5}
			& Mean & Variance & Mean & Variance\\
			\hline
			10 & & & &\\
			\hline
			15 & & & &\\
			\hline
			20 & & & &\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

We notice that the networks error decreases as the number of iterations increases.

\begin{table}[h]
	\begin{center}
		\caption{Error for 100 - 50}
		\label{tab:table2}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Iterations} & \textbf{Training Error \%} & \textbf{Test Error \%} \\
			\hline
			10 & 7.35 & 8.53\\
			& 5.81 & 6.9\\
			& 6.17 & 7.2\\
			\hline
			15 & 3.25 & 5.51\\
			& 3.18 & 5.05\\
			& 3.51 & 5.76\\
			\hline
			20 & 2.7 & 6.08\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\end{document}